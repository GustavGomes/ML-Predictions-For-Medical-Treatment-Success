{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-----------------+-------------+\n",
      "| Model Comparison               |   t-statistic 1 |   p-value 1 |\n",
      "+================================+=================+=============+\n",
      "| Decision Tree vs MLP           |        -4.081   | 0.00112306  |\n",
      "+--------------------------------+-----------------+-------------+\n",
      "| Decision Tree vs Random Forest |        -7.2481  | 4.23633e-06 |\n",
      "+--------------------------------+-----------------+-------------+\n",
      "| Decision Tree vs Naive Bayes   |         0.83182 | 0.419473    |\n",
      "+--------------------------------+-----------------+-------------+\n",
      "| MLP vs Decision Tree           |         4.081   | 0.00112306  |\n",
      "+--------------------------------+-----------------+-------------+\n",
      "| MLP vs Random Forest           |        -4.6631  | 0.000365878 |\n",
      "+--------------------------------+-----------------+-------------+\n",
      "| MLP vs Naive Bayes             |         8.32226 | 8.62187e-07 |\n",
      "+--------------------------------+-----------------+-------------+\n",
      "| Random Forest vs Decision Tree |         7.2481  | 4.23633e-06 |\n",
      "+--------------------------------+-----------------+-------------+\n",
      "| Random Forest vs MLP           |         4.6631  | 0.000365878 |\n",
      "+--------------------------------+-----------------+-------------+\n",
      "| Random Forest vs Naive Bayes   |        10.7427  | 3.81908e-08 |\n",
      "+--------------------------------+-----------------+-------------+\n",
      "| Naive Bayes vs Decision Tree   |        -0.83182 | 0.419473    |\n",
      "+--------------------------------+-----------------+-------------+\n",
      "| Naive Bayes vs MLP             |        -8.32226 | 8.62187e-07 |\n",
      "+--------------------------------+-----------------+-------------+\n",
      "| Naive Bayes vs Random Forest   |       -10.7427  | 3.81908e-08 |\n",
      "+--------------------------------+-----------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from scipy import stats\n",
    "from itertools import permutations\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load accuracy lists from pickle files\n",
    "with open('VlDecisionTree.pkl', 'rb') as f:\n",
    "    Cd4DecisionTree = pickle.load(f)\n",
    "with open('VlMlp.pkl', 'rb') as f:\n",
    "    Cd4Mlp = pickle.load(f)\n",
    "with open('VlRandomForest.pkl', 'rb') as f:\n",
    "    Cd4RandomForest = pickle.load(f)\n",
    "with open('VlNaive.pkl', 'rb') as f:\n",
    "    Cd4Naive = pickle.load(f)\n",
    "\n",
    "# Define a list of model names and their corresponding accuracy lists\n",
    "model_names = ['Decision Tree', 'MLP', 'Random Forest', 'Naive Bayes']\n",
    "accuracy_lists = [Cd4DecisionTree, Cd4Mlp, Cd4RandomForest, Cd4Naive]\n",
    "\n",
    "# Create an empty table\n",
    "table = []\n",
    "\n",
    "# Perform t-tests between all possible combinations and permutations of models\n",
    "for model1, model2 in permutations(range(len(model_names)), 2):\n",
    "    model1_name = model_names[model1]\n",
    "    model2_name = model_names[model2]\n",
    "    model1_accuracy = accuracy_lists[model1]\n",
    "    model2_accuracy = accuracy_lists[model2]\n",
    "\n",
    "    t_stats = []\n",
    "    p_values = []\n",
    "\n",
    "    # Pair the accuracy values across the models\n",
    "    for k in range(len(model1_accuracy)):\n",
    "        t_stat, p_value = stats.ttest_rel(model1_accuracy[k], model2_accuracy[k])\n",
    "        t_stats.append(t_stat)\n",
    "        p_values.append(p_value)\n",
    "\n",
    "    # Add the t-statistic and p-value to the table\n",
    "    row = [f\"{model1_name} vs {model2_name}\"] + t_stats + p_values\n",
    "    table.append(row)\n",
    "\n",
    "# Create table headers\n",
    "headers = ['Model Comparison'] + [f\"t-statistic {i+1}\" for i in range(len(model1_accuracy))] + [f\"p-value {i+1}\" for i in range(len(model1_accuracy))]\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table, headers=headers, tablefmt='grid'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
